{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Linear Regression with Direct Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 1')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(X,y):\n",
    "    # Inputs:                                                         \n",
    "    # X = N x d \n",
    "    # y = N x 1                                                               \n",
    "    # Output: \n",
    "    # w = d x 1 \n",
    "    \n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    xTranspose = np.transpose(X)\n",
    "    xTx = np.dot(xTranspose,X)\n",
    "    xTxinverse = np.linalg.inv(xTx)\n",
    "    xTxinversexT = np.dot(xTxinverse, xTranspose)\n",
    "    w = np.dot(xTxinversexT, y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # rmse = scalar value\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    left = np.transpose(ytest - np.dot(Xtest,w))\n",
    "    right = ytest-np.dot(Xtest,w)\n",
    "    paran = np.dot (left,right)\n",
    "    div = paran / 242\n",
    "    rmse = np.sqrt(div)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without intercept on train data - 138.20\n",
      "RMSE with intercept on train data - 46.77\n",
      "RMSE without intercept on test data - 297.06\n",
      "RMSE with intercept on test data - 55.36\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "x1 = np.ones((len(Xtrain),1))\n",
    "x2 = np.ones((len(Xtest),1))\n",
    "\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "w = learnOLERegression(Xtrain,ytrain)\n",
    "w_i = learnOLERegression(Xtrain_i,ytrain)\n",
    "\n",
    "rmse = testOLERegression(w,Xtrain,ytrain)\n",
    "rmse_i = testOLERegression(w_i,Xtrain_i,ytrain)\n",
    "print('RMSE without intercept on train data - %.2f'%rmse)\n",
    "print('RMSE with intercept on train data - %.2f'%rmse_i)\n",
    "\n",
    "rmse = testOLERegression(w,Xtest,ytest)\n",
    "rmse_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "print('RMSE without intercept on test data - %.2f'%rmse)\n",
    "print('RMSE with intercept on test data - %.2f'%rmse_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 2')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y):\n",
    "\n",
    "    # compute squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y      \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar value\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    Xw = np.dot(X,w)\n",
    "    yXw = y-Xw\n",
    "    transpose = np.transpose(yXw)\n",
    "    leftright = np.dot(transpose[0],yXw)\n",
    "    return .5*leftright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionGradient(w, X, y):\n",
    "\n",
    "    # compute gradient of squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y   \n",
    "    \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # gradient = d length vector (not a d x 1 matrix)\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE \n",
    "    error = regressionObjVal(w, X, y)\n",
    "    error_gradient = np.gradient(error)\n",
    "    #error_grad = np.zeros((error_gradient.shape[1],))\n",
    "    return error_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Linear Regression RMSE on train data - 167.09\n",
      "Gradient Descent Linear Regression RMSE on test data - 158.70\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "rmse = testOLERegression(w,Xtrain_i,ytrain)\n",
    "print('Gradient Descent Linear Regression RMSE on train data - %.2f'%rmse)\n",
    "rmse = testOLERegression(w,Xtest_i,ytest)\n",
    "print('Gradient Descent Linear Regression RMSE on test data - %.2f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Perceptron using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 3\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 3')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinearModel(w,Xtest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # Output:\n",
    "    # ypred = N x 1 vector of predictions\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    #print(Xtest\n",
    "    wx = np.dot(Xtest,w)\n",
    "    #print(Xtest)\n",
    "    for i in range (0,wx.shape[0]):\n",
    "        if(wx[i]>=0):\n",
    "            wx[i] = 1\n",
    "        else:\n",
    "            wx[i] = -1\n",
    "    ypred = wx\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLinearModel(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # acc = scalar values\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    ypred = predictLinearModel(w,Xtest)\n",
    "    counter = 0\n",
    "    for i in range (0,Xtest.shape[0]):\n",
    "        #print(i)\n",
    "        if(ypred[i] == ytest[i]):\n",
    "            counter+=1\n",
    "    return counter/Xtest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy on train data - 0.54\n",
      "Perceptron Accuracy on test data - 0.45\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.\n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Perceptron Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Perceptron Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - Logistic Regression Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 4')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticObjVal(w, X, y):\n",
    "\n",
    "    # compute log-loss error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar\n",
    "    \n",
    "    \n",
    "    \n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    weight_transpose = np.transpose(w)\n",
    "    sum = 0\n",
    "    for i in range (0,X.shape[0]):\n",
    "        wx = np.dot(weight_transpose,X[i])\n",
    "        theta = (1 / (1+(math.exp(-(np.dot(weight_transpose,X[i]))))))\n",
    "        y=0;\n",
    "        if(theta>=0.5):\n",
    "            y = 1\n",
    "        else:\n",
    "            y = -1\n",
    "        sum += math.log10(1+(math.exp(np.dot(-y,wx))))\n",
    "        sum=sum/X.shape[0]\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticGradient(w, X, y):\n",
    "\n",
    "    # compute the gradient of the log-loss error (vector) with respect\n",
    "    # to w (vector) for the given data X and y  \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = d length gradient vector (not a d x 1 matrix)\n",
    "\n",
    "    weight_transpose = np.transpose(w)\n",
    "    sum = 0\n",
    "    for i in range (0,X.shape[0]):\n",
    "        wx = np.dot(weight_transpose,X[i])\n",
    "        theta = (1 / (1+(math.exp(-(np.dot(weight_transpose,X[i]))))))\n",
    "        y=0;\n",
    "        if(theta>=0.5):\n",
    "            y = 1\n",
    "        else:\n",
    "            y = -1\n",
    "        bottom = 1 + math.exp(np.dot(y,wx))\n",
    "        top = y / bottom\n",
    "        final = top * X[i]\n",
    "        sum+=final\n",
    "        gradient = -sum / X.shape[0]\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticHessian(w, X, y):\n",
    "\n",
    "    # compute the Hessian of the log-loss error (matrix) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # Hessian = d x d matrix\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    weight_transpose = np.transpose(w)\n",
    "    sum = 0\n",
    "    for i in range (0, X.shape[0]):\n",
    "        wx = np.dot (weight_transpose,X[i])\n",
    "        theta = (1 / (1+(math.exp(-(np.dot(weight_transpose,X[i]))))))\n",
    "        y=0;\n",
    "        if(theta>=0.5):\n",
    "            y = 1\n",
    "        else:\n",
    "            y = -1\n",
    "        top = math.exp(np.dot(y,wx))\n",
    "        bottom = math.pow((1 + math.exp(np.dot(y,wx))),2)\n",
    "        left = top / bottom\n",
    "        right = np.dot(X[i],np.transpose(X[i]))\n",
    "        inner = np.dot(left,right)\n",
    "        sum +=inner\n",
    "    hessian = -sum/X.shape[0]\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -1.10127802 -0.51150853]\n",
      "[-0.22765726 -0.42257658 -0.1029574 ]\n",
      "[-0.22765726 -0.42257658 -0.1029574 ]\n",
      "[-0.22472813 -0.41585436 -0.09983575]\n",
      "[-0.21338656 -0.38998947 -0.08795825]\n",
      "[-0.17366024 -0.30165652 -0.04927547]\n",
      "[-0.17366024 -0.30165652 -0.04927547]\n",
      "[-0.17315045 -0.30055462 -0.04885752]\n",
      "[-0.17112704 -0.29618779 -0.0472071 ]\n",
      "[-0.1632809  -0.27935851 -0.04093785]\n",
      "[-0.13556858 -0.2213488  -0.02059704]\n",
      "[-0.13556858 -0.2213488  -0.02059704]\n",
      "[-0.13521565 -0.22062989 -0.02038774]\n",
      "[-0.13381376 -0.21777832 -0.01956122]\n",
      "[-0.1283608  -0.20674913 -0.01642109]\n",
      "[-0.10884685 -0.16815186 -0.00622092]\n",
      "[-0.10884685 -0.16815186 -0.00622092]\n",
      "[-0.1086062  -0.16768752 -0.00612319]\n",
      "[-0.1076493  -0.16584348 -0.00573707]\n",
      "[-0.10391186 -0.15867626 -0.00426789]\n",
      "[-0.09031331 -0.13309887  0.00053105]\n",
      "[-0.09031331 -0.13309887  0.00053105]\n",
      "[-0.09015113 -0.13280021  0.00057408]\n",
      "[-0.08950554 -0.13161255  0.00074416]\n",
      "[-0.08697273 -0.12697182  0.00139258]\n",
      "[-0.07759267 -0.11005729  0.00352438]\n",
      "[-0.07759267 -0.11005729  0.00352438]\n",
      "[-0.0774839  -0.10986448  0.00354243]\n",
      "[-0.07705048 -0.10909676  0.00361381]\n",
      "[-0.07534284 -0.10608161  0.00388631]\n",
      "[-0.0689116  -0.09486803  0.0047847 ]\n",
      "[-0.0689116  -0.09486803  0.0047847 ]\n",
      "[-0.06883857 -0.09474235  0.0047919 ]\n",
      "[-0.06854725 -0.09424138  0.00482036]\n",
      "[-0.06739528 -0.09226511  0.00492892]\n",
      "[-0.06299296 -0.08478471  0.00528411]\n",
      "[-0.04827568 -0.06069295  0.00575049]\n",
      "[-0.04827568 -0.06069295  0.00575049]\n",
      "[-0.04810166 -0.06041962  0.00573849]\n",
      "[-0.04741233 -0.05933916  0.0056896 ]\n",
      "[-0.0447604  -0.05521671  0.00548151]\n",
      "[-0.03566114 -0.0415203   0.0045249 ]\n",
      "[-0.03566114 -0.0415203   0.0045249 ]\n",
      "[-0.03555802 -0.04137031  0.0045101 ]\n",
      "[-0.03514879 -0.04077602  0.00445094]\n",
      "[-0.03356243 -0.03848766  0.00421542]\n",
      "[-0.02795575 -0.03060919  0.00330685]\n",
      "[-0.02795575 -0.03060919  0.00330685]\n",
      "[-0.02789341 -0.03052394  0.00329636]\n",
      "[-0.02764559 -0.03018545  0.00325451]\n",
      "[-0.02667804 -0.02887078  0.00308915]\n",
      "[-0.02316167 -0.02418988  0.00246345]\n",
      "[-0.02316167 -0.02418988  0.00246345]\n",
      "[-0.02312304 -0.02413951  0.0024568 ]\n",
      "[-0.02296921 -0.02393917  0.00243026]\n",
      "[-0.02236496 -0.02315531  0.00232537]\n",
      "[-0.02011567 -0.02028237  0.00192625]\n",
      "[-0.02011567 -0.02028237  0.00192625]\n",
      "[-0.02009123 -0.02025163  0.00192212]\n",
      "[-0.01999376 -0.02012916  0.00190563]\n",
      "[-0.019609   -0.01964712  0.00184031]\n",
      "[-0.01814867 -0.01783829  0.00158916]\n",
      "[-0.01814867 -0.01783829  0.00158916]\n",
      "[-0.01813296 -0.01781906  0.00158658]\n",
      "[-0.01807027 -0.01774234  0.00157628]\n",
      "[-0.01782186 -0.01743894  0.00153537]\n",
      "[-0.0168647  -0.01627943  0.00137646]\n",
      "[-0.01355922 -0.0124004   0.00081637]\n",
      "[-0.01355922 -0.0124004   0.00081637]\n",
      "[-0.01352309 -0.01235942  0.0008115 ]\n",
      "[-0.01337961 -0.01219694  0.00079211]\n",
      "[-0.01282191 -0.01156938  0.00071638]\n",
      "[-0.01082976 -0.0093825   0.00044234]\n",
      "[-0.01082976 -0.0093825   0.00044234]\n",
      "[-0.01080882 -0.00936012  0.00044003]\n",
      "[-0.0107255  -0.00927113  0.00043083]\n",
      "[-0.01039903 -0.0089241   0.00039468]\n",
      "[-0.00919596 -0.00766807  0.00026013]\n",
      "[-0.00919596 -0.00766807  0.00026013]\n",
      "[-0.0091838  -0.00765562  0.000259  ]\n",
      "[-0.00913536 -0.00760604  0.00025449]\n",
      "[-0.0089443  -0.0074111   0.00023666]\n",
      "[-0.00822168 -0.00668282  0.00016873]\n",
      "[-0.00822168 -0.00668282  0.00016873]\n",
      "[-0.00821463 -0.00667581  0.00016816]\n",
      "[-0.00818651 -0.00664784  0.00016585]\n",
      "[-0.00807503 -0.00653721  0.0001567 ]\n",
      "[-0.00764502 -0.00611383  0.0001212 ]\n",
      "[-6.15382125e-03 -4.69002806e-03 -2.69033182e-06]\n",
      "[-6.15382125e-03 -4.69002806e-03 -2.69033182e-06]\n",
      "[-6.13957128e-03 -4.67687952e-03 -3.34905071e-06]\n",
      "[-6.08291871e-03 -4.62467434e-03 -5.97342921e-06]\n",
      "[-5.86176616e-03 -4.42194316e-03 -1.62973609e-05]\n",
      "[-5.05847365e-03 -3.70041143e-03 -5.45309448e-05]\n",
      "[-5.05847365e-03 -3.70041143e-03 -5.45309448e-05]\n",
      "[-5.05141569e-03 -3.69421066e-03 -5.47329829e-05]\n",
      "[-5.02328707e-03 -3.66951719e-03 -5.55397452e-05]\n",
      "[-4.91240596e-03 -3.57247430e-03 -5.87433805e-05]\n",
      "[-4.49392099e-03 -3.21060190e-03 -7.11162416e-05]\n",
      "[-4.49392099e-03 -3.21060190e-03 -7.11162416e-05]\n",
      "[-4.49046753e-03 -3.20765370e-03 -7.11855292e-05]\n",
      "[-4.47668146e-03 -3.19588945e-03 -7.14625173e-05]\n",
      "[-4.42197846e-03 -3.14928603e-03 -7.25676909e-05]\n",
      "[-4.21006276e-03 -2.96992557e-03 -7.69329602e-05]\n",
      "[-3.46298180e-03 -2.35342011e-03 -9.29556558e-05]\n",
      "[-3.46298180e-03 -2.35342011e-03 -9.29556558e-05]\n",
      "[-3.45707637e-03 -2.34868123e-03 -9.29489241e-05]\n",
      "[-3.43355917e-03 -2.32982594e-03 -9.29236723e-05]\n",
      "[-3.34114092e-03 -2.25598402e-03 -9.28477009e-05]\n",
      "[-2.99655671e-03 -1.98437220e-03 -9.28429144e-05]\n",
      "[-2.99655671e-03 -1.98437220e-03 -9.28429144e-05]\n",
      "[-2.99408647e-03 -1.98245301e-03 -9.28211612e-05]\n",
      "[-2.98422663e-03 -1.97479574e-03 -9.27346321e-05]\n",
      "[-2.94512258e-03 -1.94447625e-03 -9.23960806e-05]\n",
      "[-2.79394028e-03 -1.82800641e-03 -9.11521859e-05]\n",
      "[-2.26517371e-03 -1.43058643e-03 -8.73736534e-05]\n",
      "[-2.26517371e-03 -1.43058643e-03 -8.73736534e-05]\n",
      "[-2.26161164e-03 -1.42798058e-03 -8.72861256e-05]\n",
      "[-2.24742099e-03 -1.41760681e-03 -8.69382812e-05]\n",
      "[-2.19157025e-03 -1.37689521e-03 -8.55821779e-05]\n",
      "[-1.98209316e-03 -1.22590800e-03 -8.06635985e-05]\n",
      "[-1.98209316e-03 -1.22590800e-03 -8.06635985e-05]\n",
      "[-1.98083464e-03 -1.22501147e-03 -8.06275303e-05]\n",
      "[-1.97580878e-03 -1.22143222e-03 -8.04836074e-05]\n",
      "[-1.95583619e-03 -1.20722432e-03 -7.99134544e-05]\n",
      "[-1.87800060e-03 -1.15209976e-03 -7.77179544e-05]\n",
      "[-1.59717675e-03 -9.56610186e-04 -7.00983246e-05]\n",
      "[-1.59717675e-03 -9.56610186e-04 -7.00983246e-05]\n",
      "[-1.59559033e-03 -9.55525754e-04 -7.00421570e-05]\n",
      "[-1.58926080e-03 -9.51200844e-04 -6.98182735e-05]\n",
      "[-1.56419916e-03 -9.34104577e-04 -6.89351534e-05]\n",
      "[-1.46793873e-03 -8.68863303e-04 -6.55907129e-05]\n",
      "[-1.13976179e-03 -6.51863353e-04 -5.46498928e-05]\n",
      "[-1.13976179e-03 -6.51863353e-04 -5.46498928e-05]\n",
      "[-1.13809312e-03 -6.50788753e-04 -5.45749778e-05]\n",
      "[-1.13144335e-03 -6.46508755e-04 -5.42767751e-05]\n",
      "[-1.10523896e-03 -6.29679494e-04 -5.31068792e-05]\n",
      "[-1.00647245e-03 -5.66789065e-04 -4.87696268e-05]\n",
      "[-1.00647245e-03 -5.66789065e-04 -4.87696268e-05]\n",
      "[-1.00603869e-03 -5.66515229e-04 -4.87497367e-05]\n",
      "[-1.00430557e-03 -5.65421256e-04 -4.86702881e-05]\n",
      "[-9.97403506e-04 -5.61067272e-04 -4.83542697e-05]\n",
      "[-9.70275991e-04 -5.43996735e-04 -4.71180586e-05]\n",
      "[-8.69082890e-04 -4.80930295e-04 -4.25856545e-05]\n",
      "[-8.69082890e-04 -4.80930295e-04 -4.25856545e-05]\n",
      "[-8.68663449e-04 -4.80671408e-04 -4.25659383e-05]\n",
      "[-8.66987748e-04 -4.79637304e-04 -4.24871947e-05]\n",
      "[-8.60317836e-04 -4.75523915e-04 -4.21741478e-05]\n",
      "[-8.34157142e-04 -4.59432879e-04 -4.09521496e-05]\n",
      "[-7.37367597e-04 -4.00508344e-04 -3.65083989e-05]\n",
      "[-7.37367597e-04 -4.00508344e-04 -3.65083989e-05]\n",
      "[-7.36994080e-04 -4.00283282e-04 -3.64903950e-05]\n",
      "[-7.35501937e-04 -3.99384342e-04 -3.64184930e-05]\n",
      "[-7.29564067e-04 -3.95809442e-04 -3.61326954e-05]\n",
      "[-7.06296615e-04 -3.81838012e-04 -3.50178563e-05]\n",
      "[-6.20532619e-04 -3.30863817e-04 -3.09753369e-05]\n",
      "[-6.20532619e-04 -3.30863817e-04 -3.09753369e-05]\n",
      "[-6.20226923e-04 -3.30683971e-04 -3.09602816e-05]\n",
      "[-6.19005673e-04 -3.29965594e-04 -3.09001515e-05]\n",
      "[-6.14145080e-04 -3.27108186e-04 -3.06610787e-05]\n",
      "[-5.95087722e-04 -3.15931992e-04 -2.97274847e-05]\n",
      "[-5.24679585e-04 -2.75029220e-04 -2.63285036e-05]\n",
      "[-5.24679585e-04 -2.75029220e-04 -2.63285036e-05]\n",
      "[-5.24448517e-04 -2.74896231e-04 -2.63169462e-05]\n",
      "[-5.23525277e-04 -2.74364939e-04 -2.62707780e-05]\n",
      "[-5.19848803e-04 -2.72250348e-04 -2.60870848e-05]\n",
      "[-5.05403312e-04 -2.63958832e-04 -2.53677158e-05]\n",
      "[-4.51582530e-04 -2.33313829e-04 -2.27202140e-05]\n",
      "[-4.51582530e-04 -2.33313829e-04 -2.27202140e-05]\n",
      "[-4.51419378e-04 -2.33221661e-04 -2.27119765e-05]\n",
      "[-4.50767369e-04 -2.32853361e-04 -2.26790622e-05]\n",
      "[-4.48168880e-04 -2.31386150e-04 -2.25479719e-05]\n",
      "[-4.37926079e-04 -2.25611973e-04 -2.20325507e-05]\n",
      "[-3.99275170e-04 -2.03960783e-04 -2.01061651e-05]\n",
      "[-3.99275170e-04 -2.03960783e-04 -2.01061651e-05]\n",
      "[-3.99165928e-04 -2.03899963e-04 -2.01006225e-05]\n",
      "[-3.98729260e-04 -2.03656867e-04 -2.00784703e-05]\n",
      "[-3.96987432e-04 -2.02687467e-04 -1.99901477e-05]\n",
      "[-3.90096967e-04 -1.98857146e-04 -1.96413922e-05]\n",
      "[-3.63726641e-04 -1.84266130e-04 -1.83159707e-05]\n",
      "[-2.75097178e-04 -1.36080069e-04 -1.39626196e-05]\n",
      "[-2.75097178e-04 -1.36080069e-04 -1.39626196e-05]\n",
      "[-2.74865189e-04 -1.35956148e-04 -1.39504284e-05]\n",
      "[-2.73939216e-04 -1.35461620e-04 -1.39017838e-05]\n",
      "[-2.70266803e-04 -1.33501911e-04 -1.37091126e-05]\n",
      "[-2.56068853e-04 -1.25949682e-04 -1.29680196e-05]\n",
      "[-2.06429116e-04 -9.98670653e-05 -1.04234983e-05]\n",
      "[-2.06429116e-04 -9.98670653e-05 -1.04234983e-05]\n",
      "[-2.06320145e-04 -9.98104833e-05 -1.04177934e-05]\n",
      "[-2.05884840e-04 -9.95844840e-05 -1.03950082e-05]\n",
      "[-2.04152890e-04 -9.86857179e-05 -1.03044189e-05]\n",
      "[-1.97371197e-04 -9.51729734e-05 -9.95072132e-06]\n",
      "[-1.72446155e-04 -8.23550309e-05 -8.66459366e-06]\n",
      "[-1.72446155e-04 -8.23550309e-05 -8.66459366e-06]\n",
      "[-1.72398147e-04 -8.23305092e-05 -8.66209703e-06]\n",
      "[-1.72206249e-04 -8.22324972e-05 -8.65211841e-06]\n",
      "[-1.71440814e-04 -8.18416418e-05 -8.61233017e-06]\n",
      "[-1.68413312e-04 -8.02971381e-05 -8.45517766e-06]\n",
      "[-1.56833996e-04 -7.44114534e-05 -7.85735746e-06]\n",
      "[-1.18012839e-04 -5.49502734e-05 -5.88992691e-06]\n",
      "[-1.18012839e-04 -5.49502734e-05 -5.88992691e-06]\n",
      "[-1.17945163e-04 -5.49167995e-05 -5.88641130e-06]\n",
      "[-1.17674850e-04 -5.47831119e-05 -5.87237150e-06]\n",
      "[-1.16599841e-04 -5.42516758e-05 -5.81657273e-06]\n",
      "[-1.12398036e-04 -5.21780288e-05 -5.59903289e-06]\n",
      "[-9.70638877e-05 -4.46603682e-05 -4.81266425e-06]\n",
      "[-9.70638877e-05 -4.46603682e-05 -4.81266425e-06]\n",
      "[-9.70413513e-05 -4.46493873e-05 -4.81150466e-06]\n",
      "[-9.69512587e-05 -4.46054910e-05 -4.80686928e-06]\n",
      "[-9.65917316e-05 -4.44303459e-05 -4.78837559e-06]\n",
      "[-9.51670263e-05 -4.37367504e-05 -4.71516009e-06]\n",
      "[-8.96770694e-05 -4.10709801e-05 -4.43407530e-06]\n",
      "[-7.07289917e-05 -3.19610788e-05 -3.47665949e-06]\n",
      "[-7.07289917e-05 -3.19610788e-05 -3.47665949e-06]\n",
      "[-7.07038292e-05 -3.19490936e-05 -3.47537626e-06]\n",
      "[-7.06032697e-05 -3.19011987e-05 -3.47024834e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.02024717e-05 -3.17103486e-05 -3.44981681e-06]\n",
      "[-6.86220874e-05 -3.09585007e-05 -3.36935761e-06]\n",
      "[-6.26510755e-05 -2.81281115e-05 -3.06687244e-06]\n",
      "[-6.26510755e-05 -2.81281115e-05 -3.06687244e-06]\n",
      "[-6.26441600e-05 -2.81248441e-05 -3.06652182e-06]\n",
      "[-6.26165056e-05 -2.81117784e-05 -3.06511979e-06]\n",
      "[-6.25060111e-05 -2.80595773e-05 -3.05951843e-06]\n",
      "[-6.20659948e-05 -2.78517573e-05 -3.03722097e-06]\n",
      "[-6.03369283e-05 -2.70360179e-05 -2.94973410e-06]\n",
      "[-5.38925246e-05 -2.40087647e-05 -2.62552446e-06]\n",
      "[-5.38925246e-05 -2.40087647e-05 -2.62552446e-06]\n",
      "[-5.38854903e-05 -2.40054732e-05 -2.62517002e-06]\n",
      "[-5.38573624e-05 -2.39923121e-05 -2.62375275e-06]\n",
      "[-5.37449983e-05 -2.39397409e-05 -2.61809173e-06]\n",
      "[-5.32978994e-05 -2.37306249e-05 -2.59557593e-06]\n",
      "[-5.15466643e-05 -2.29125648e-05 -2.50753076e-06]\n",
      "[-4.51023363e-05 -1.99168424e-05 -2.18555664e-06]\n",
      "[-4.51023363e-05 -1.99168424e-05 -2.18555664e-06]\n",
      "[-4.50958007e-05 -1.99138177e-05 -2.18522940e-06]\n",
      "[-4.50696679e-05 -1.99017237e-05 -2.18392092e-06]\n",
      "[-4.49652894e-05 -1.98534219e-05 -2.17869523e-06]\n",
      "[-4.45502042e-05 -1.96614029e-05 -2.15792313e-06]\n",
      "[-4.29280964e-05 -1.89120054e-05 -2.07688717e-06]\n",
      "[-3.70128113e-05 -1.61932200e-05 -1.78326527e-06]\n",
      "[-3.70128113e-05 -1.61932200e-05 -1.78326527e-06]\n",
      "[-3.70073045e-05 -1.61907008e-05 -1.78299129e-06]\n",
      "[-3.69852855e-05 -1.61806279e-05 -1.78189580e-06]\n",
      "[-3.68973415e-05 -1.61403998e-05 -1.77752088e-06]\n",
      "[-3.65476656e-05 -1.59805000e-05 -1.76013290e-06]\n",
      "[-3.51820068e-05 -1.53568160e-05 -1.69233502e-06]\n",
      "[-3.02139125e-05 -1.30992681e-05 -1.44719649e-06]\n",
      "[-3.02139125e-05 -1.30992681e-05 -1.44719649e-06]\n",
      "[-3.02097002e-05 -1.30973627e-05 -1.44698825e-06]\n",
      "[-3.01928569e-05 -1.30897437e-05 -1.44615562e-06]\n",
      "[-3.01255783e-05 -1.30593129e-05 -1.44283004e-06]\n",
      "[-2.98579691e-05 -1.29383057e-05 -1.42960691e-06]\n",
      "[-2.88112424e-05 -1.24655403e-05 -1.37795948e-06]\n",
      "[-2.49806728e-05 -1.07431304e-05 -1.18996048e-06]\n",
      "[-2.49806728e-05 -1.07431304e-05 -1.18996048e-06]\n",
      "[-2.49777172e-05 -1.07418067e-05 -1.18981526e-06]\n",
      "[-2.49658982e-05 -1.07365138e-05 -1.18923455e-06]\n",
      "[-2.49186786e-05 -1.07153683e-05 -1.18691464e-06]\n",
      "[-2.47306969e-05 -1.06312077e-05 -1.17768171e-06]\n",
      "[-2.39929286e-05 -1.03012107e-05 -1.14148626e-06]\n",
      "[-2.12566600e-05 -9.08173828e-06 -1.00781724e-06]\n",
      "[-2.12566600e-05 -9.08173828e-06 -1.00781724e-06]\n",
      "[-2.12547240e-05 -9.08087829e-06 -1.00772262e-06]\n",
      "[-2.12469821e-05 -9.07743913e-06 -1.00734425e-06]\n",
      "[-2.12160425e-05 -9.06369571e-06 -1.00583221e-06]\n",
      "[-2.10927369e-05 -9.00893245e-06 -9.99807406e-07]\n",
      "[-2.06066804e-05 -8.79321042e-06 -9.76077785e-07]\n",
      "[-1.87725017e-05 -7.98136093e-06 -8.86813819e-07]\n",
      "[-1.87725017e-05 -7.98136093e-06 -8.86813819e-07]\n",
      "[-1.87712944e-05 -7.98082783e-06 -8.86755058e-07]\n",
      "[-1.87664660e-05 -7.97869578e-06 -8.86520057e-07]\n",
      "[-1.87471647e-05 -7.97017333e-06 -8.85580691e-07]\n",
      "[-1.86701592e-05 -7.93617564e-06 -8.81833457e-07]\n",
      "[-1.83653045e-05 -7.80164690e-06 -8.67006868e-07]\n",
      "[-1.71951203e-05 -7.28622001e-06 -8.10217443e-07]\n",
      "[-1.32171270e-05 -5.54643797e-06 -6.18652168e-07]\n",
      "[-1.32171270e-05 -5.54643797e-06 -6.18652168e-07]\n",
      "[-1.32146950e-05 -5.54538123e-06 -6.18534818e-07]\n",
      "[-1.32049718e-05 -5.54115634e-06 -6.18065643e-07]\n",
      "[-1.31661506e-05 -5.52428929e-06 -6.16192581e-07]\n",
      "[-1.30120096e-05 -5.45733879e-06 -6.08758187e-07]\n",
      "[-1.24133697e-05 -5.19764147e-06 -5.79925827e-07]\n",
      "[-1.02829931e-05 -4.27783486e-06 -4.77858849e-07]\n",
      "[-1.02829931e-05 -4.27783486e-06 -4.77858849e-07]\n",
      "[-1.02818800e-05 -4.27735629e-06 -4.77805582e-07]\n",
      "[-1.02774287e-05 -4.27544254e-06 -4.77592573e-07]\n",
      "[-1.02596428e-05 -4.26779622e-06 -4.76741507e-07]\n",
      "[-1.01888081e-05 -4.23734889e-06 -4.73352665e-07]\n",
      "[-9.91034906e-06 -4.11773900e-06 -4.60040863e-07]\n",
      "[-8.87088836e-06 -3.67244291e-06 -4.10494017e-07]\n",
      "Logistic Regression Accuracy on train data - 0.00\n",
      "Logistic Regression Accuracy on test data - 0.00\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(logisticObjVal, w_init, jac=logisticGradient, hess=logisticHessian, args=args,method='Newton-CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = np.reshape(w,[len(w),1])\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Logistic Regression Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Logistic Regression Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - Support Vector Machines Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROBLEM 5')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGDSVM(X,y,T,eta=0.01):\n",
    "    # learn a linear SVM by implementing the SGD algorithm\n",
    "    #\n",
    "    # Inputs:\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # T = number of iterations\n",
    "    # eta = learning rate\n",
    "    # Output:\n",
    "    # weight vector, w = d x 1\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    w = np.zeros([X.shape[1],1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "w = trainSGDSVM(Xtrain_i,ytrain,100,0.01)\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('SVM Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('SVM Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 - Plotting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Problem 6')\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundaries(w,X,y):\n",
    "    # plotting boundaries\n",
    "\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    x1 = np.linspace(mn[1],mx[1],100)\n",
    "    x2 = np.linspace(mn[2],mx[2],100)\n",
    "    xx1,xx2 = np.meshgrid(x1,x2)\n",
    "    xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "    xx[:,0] = xx1.ravel()\n",
    "    xx[:,1] = xx2.ravel()\n",
    "    xx_i = np.concatenate((np.ones((xx.shape[0],1)), xx), axis=1)\n",
    "    ypred = predictLinearModel(w,xx_i)\n",
    "    ax.contourf(x1,x2,ypred.reshape((x1.shape[0],x2.shape[0])),alpha=0.3,cmap='cool')\n",
    "    ax.scatter(X[:,1],X[:,2],c=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = np.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "# Replace next three lines with code for learning w using the three methods\n",
    "w_perceptron = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_logistic = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_svm = np.zeros((Xtrain_i.shape[1],1))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = plt.subplot(1,3,1)\n",
    "plotBoundaries(w_perceptron,Xtrain_i,ytrain)\n",
    "ax.set_title('Perceptron')\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plotBoundaries(w_logistic,Xtrain_i,ytrain)\n",
    "ax.set_title('Logistic Regression')\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "plotBoundaries(w_svm,Xtrain_i,ytrain)\n",
    "ax.set_title('SVM')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
